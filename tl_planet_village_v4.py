# -*- coding: utf-8 -*-
"""TL_planet_village_V4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P4clc9ieodafaNk1DTdIxWXFNnqtUyzj
"""

import tensorflow as tf
import tensorflow_datasets as tfds
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau

!pip freeze > requirements.yml

"""1. Carga y Exploración del Dataset"""

# Load dataset
dataset, info = tfds.load('plant_village', with_info=True, as_supervised=True)
num_classes = info.features['label'].num_classes
class_names = info.features['label'].names
print(f"Clases: {num_classes}")
print(info)

"""El conjunto de datos plant_leaves contiene 4,502 imágenes de hojas de plantas, clasificadas en 22 categorías según la especie y el estado de salud.

2. Preprocesamiento y División de Datos

Dividimos los datos en conjuntos de entrenamiento, validación y prueba. Además, aplicamos técnicas de aumento de datos para mejorar la generalización del modelo.
"""

# Split dataset
all_data = dataset['train']
total_examples = info.splits['train'].num_examples

train_size = int(0.7 * total_examples)
val_size = int(0.15 * total_examples)
test_size = total_examples - train_size - val_size

train_data = all_data.take(train_size)
val_data = all_data.skip(train_size).take(val_size)
test_data = all_data.skip(train_size + val_size)

# Show example images with label names
plt.figure(figsize=(10, 10))
for i, (image, label) in enumerate(all_data.take(9)):
    plt.subplot(3, 3, i + 1)
    plt.imshow(image)
    plt.title(f"Label: {class_names[label.numpy()]}", fontsize=8)
    plt.axis('off')
plt.tight_layout()
plt.show()

# Preprocess data
def format_image(image, label):
    image = tf.image.resize(image, (224, 224))
    image = tf.cast(image, tf.float32) / 255.0
    return image, label

batch_size = 32
train_batches = (train_data.map(format_image)
                 .shuffle(1000)
                 .batch(batch_size)
                 .prefetch(tf.data.AUTOTUNE))
val_batches = (val_data.map(format_image)
               .batch(batch_size)
               .prefetch(tf.data.AUTOTUNE))
test_batches = (test_data.map(format_image)
                .batch(batch_size)
                .prefetch(tf.data.AUTOTUNE))

"""3. Construcción del Modelo con Transfer Learning

Utilizamos un modelo preentrenado (por ejemplo, EfficientNetB0) y añadimos capas personalizadas para adaptarlo a nuestro conjunto de datos.
"""

# Load pretrained model (MobileNetV2)
base_model = tf.keras.applications.MobileNetV2(input_shape=(224, 224, 3),
                                               include_top=False,
                                               weights='imagenet')
base_model.trainable = False

# Build model
model = tf.keras.Sequential([
    base_model,
    tf.keras.layers.GlobalAveragePooling2D(),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(num_classes, activation='softmax')
])

# Define the callbacks
early_stop = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True,
    verbose=1
)

model_checkpoint = ModelCheckpoint(
    filepath='best_model.keras',
    monitor='val_loss',
    save_best_only=True,
    verbose=1
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=3,
    min_lr=1e-6,
    verbose=1
)

# Compile the model (assuming 'model' is already defined)
model.compile(
    optimizer=tf.keras.optimizers.Adam(),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train the model with the callbacks
history = model.fit(
    train_batches,
    epochs=50,
    validation_data=val_batches,
    callbacks=[early_stop, model_checkpoint, reduce_lr]
)

# Plot accuracy and loss
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.legend()
plt.title('Accuracy Over Epochs')

plt.show()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.legend()
plt.title('Loss Over Epochs')

plt.show()

# Evaluate on test set
y_true = []
y_pred = []

for images, labels in test_batches:
    preds = model.predict(images)
    y_true.extend(labels.numpy())
    y_pred.extend(np.argmax(preds, axis=1))

# Confusion matrix
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(12, 10))
sns.heatmap(cm, annot=False, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# Classification report
print(classification_report(y_true, y_pred))

# Predict on random test images with label names
for image, label in test_data.shuffle(1000).take(5):
    img_resized = tf.image.resize(image, (224, 224)) / 255.0
    pred = model.predict(tf.expand_dims(img_resized, axis=0))
    predicted_label = np.argmax(pred)
    plt.imshow(image)
    plt.title(f"True: {label.numpy()} ({class_names[label.numpy()]}) | Predicted: {predicted_label} ({class_names[predicted_label]})", fontsize=10)
    plt.axis('off')
    plt.show()

# Save model
model.save('plant_leaves_classifier_expert.keras')